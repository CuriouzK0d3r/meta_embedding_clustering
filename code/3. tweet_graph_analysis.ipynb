{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import *\n",
    "\n",
    "num_grams = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved data from previous step\n",
    "print(\"Loading combined vectors\")\n",
    "combined_vecs = load_bin(\"preprocessed/combined_vecs.pkl\")\n",
    "print(len(combined_vecs))\n",
    "print(\"Loading texts\")\n",
    "base_texts = load_json(\"preprocessed/texts.json\")\n",
    "print(len(base_texts))\n",
    "print(\"Loading sns\")\n",
    "screen_names = load_json(\"preprocessed/sns.json\")\n",
    "print(len(screen_names))\n",
    "print(\"Loading ids\")\n",
    "id_strs = load_json(\"preprocessed/ids.json\")\n",
    "print(len(id_strs))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This cell performs the clustering\n",
    "\n",
    "# Parameters used by the algorithm\n",
    "# See the blog post for details about these\n",
    "batch_size = 10000\n",
    "total_samples = 10000\n",
    "edge_ratio = 3\n",
    "min_cluster_size = 50\n",
    "merge_similarity = 0.98\n",
    "similarity_threshold = 0\n",
    "\n",
    "msg = \"TARGET:\" + str(total_samples) \n",
    "msg += \" BS:\" + str(batch_size)\n",
    "msg += \" ER:\" + str(edge_ratio)\n",
    "msg += \" STH:\" + str(similarity_threshold)\n",
    "msg += \" MSIM:\" + str(merge_similarity)\n",
    "msg += \" MCS:\" + str(min_cluster_size)\n",
    "print(msg)\n",
    "\n",
    "# State information is saved in these variables\n",
    "# and used in later cells to generate output\n",
    "centers = []\n",
    "center_labels = []\n",
    "center_words = []\n",
    "center_ngrams = []\n",
    "center_svo = []\n",
    "center_sentiment = []\n",
    "center_sns = []\n",
    "center_sizes = []\n",
    "center_urls = []\n",
    "center_tweets = []\n",
    "\n",
    "vec_label = []\n",
    "item_mod = {}\n",
    "\n",
    "ncindices = []\n",
    "final_mapping = []\n",
    "\n",
    "\n",
    "used = set()\n",
    "mod_num = 0\n",
    "passes = 0\n",
    "merges = 0\n",
    "total_processed = 0\n",
    "finished = False\n",
    "\n",
    "# If this is true, samples will be randomly selected from the whole set\n",
    "# If this is false, samples will be selected sequentially starting from current_index\n",
    "get_random = True\n",
    "\n",
    "# If this is true, the start index for sequential sampling will be randomly selected\n",
    "start_random = True\n",
    "current_index = 0\n",
    "if start_random == True:\n",
    "    end_ind = max(0, (len(base_texts) - total_samples+1))\n",
    "    if end_ind > 0:\n",
    "        current_index = random.randint(0, end_ind)\n",
    "\n",
    "if get_random == True:\n",
    "    print(\"Using random sampling.\")\n",
    "else:\n",
    "    print(\"Using sequential sampling, starting at index: \" + str(current_index))\n",
    "\n",
    "while finished == False:    \n",
    "    if total_processed < total_samples:\n",
    "        num_to_add = batch_size - len(ncindices)\n",
    "        \n",
    "        # If this got stuck and didn't create any new clusters, truncate the list and add new stuff\n",
    "        if num_to_add == 0 and passes > 0:\n",
    "            samples_left = len(base_texts) - current_index\n",
    "            num_to_remove = min(samples_left, round(batch_size/10))\n",
    "            ncindices = ncindices[num_to_remove:]\n",
    "            num_to_add = num_to_remove\n",
    "        \n",
    "        # Add new samples to batch\n",
    "        count = 0\n",
    "        if get_random == True:\n",
    "            while count < num_to_add:\n",
    "                rindex = random.randint(0,len(base_texts)-1)\n",
    "                if rindex not in used:\n",
    "                    used.add(rindex)\n",
    "                    ncindices.append(rindex)\n",
    "                    count += 1\n",
    "        else:\n",
    "            while count < num_to_add:\n",
    "                ncindices.append(current_index)\n",
    "                current_index += 1\n",
    "                if current_index > len(base_texts):\n",
    "                    finished = True\n",
    "                    break\n",
    "                count += 1\n",
    "\n",
    "    ncvectors = [combined_vecs[i] for i in ncindices]\n",
    "    \n",
    "    clusters, mapping = make_text_clusters(ncvectors, \n",
    "                                           edge_ratio=edge_ratio, \n",
    "                                           threshold=similarity_threshold)\n",
    "\n",
    "    # Build mapping for gephi visualization\n",
    "    nodes_to_omit = set()\n",
    "    for mod, idl in clusters.items():\n",
    "        if len(idl) <= min_cluster_size:\n",
    "            for node in idl:\n",
    "                nodes_to_omit.add(node)\n",
    "    for m in mapping:\n",
    "        x, y, c = m\n",
    "        if x not in nodes_to_omit and y not in nodes_to_omit:\n",
    "            final_mapping.append([ncindices[x], ncindices[y], c])\n",
    "            \n",
    "    # Renumber clusters to include actual data indices\n",
    "    clustered = set()\n",
    "    rclusters = {}\n",
    "    for mod, idl in clusters.items():\n",
    "        rclusters[mod] = []\n",
    "        for i in idl:\n",
    "            orig_index = ncindices[i]\n",
    "            rclusters[mod].append(orig_index)\n",
    "            clustered.add(orig_index)\n",
    "\n",
    "    # Create a list of unclustered samples\n",
    "    not_clustered = set(ncindices).difference(clustered)\n",
    "    new_ncindices = list(not_clustered)\n",
    "    \n",
    "    # Iterate through identified clusters\n",
    "    for mod, idl in sorted(rclusters.items()):\n",
    "        # Check if cluster matches min_cluster_size\n",
    "        if len(idl) >= min_cluster_size:\n",
    "            texts = [base_texts[index] for index in idl]\n",
    "            sns = [screen_names[index] for index in idl]\n",
    "            ids = [id_strs[index] for index in idl]\n",
    "            vectors = [combined_vecs[index] for index in idl]\n",
    "            center = get_cluster_center(vectors)\n",
    "            \n",
    "            # Check if this cluster has a similar center to any other already found\n",
    "            cluster_mod = None\n",
    "            new_cluster = False\n",
    "            sc = 0\n",
    "            if len(centers) > 1:\n",
    "                scores = fast_cosine_matrix(np.array(center), np.array(centers))\n",
    "                sc = np.max(scores)\n",
    "\n",
    "            if sc >= merge_similarity:\n",
    "                cluster_mod = np.argmax(scores)\n",
    "                merges += 1\n",
    "            else:\n",
    "                cluster_mod = mod_num\n",
    "                new_cluster = True\n",
    "                mod_num += 1\n",
    "\n",
    "            # This is used for gephi visualization\n",
    "            for c, item in enumerate(idl):\n",
    "                item_mod[item] = [cluster_mod]\n",
    "\n",
    "            # Measure the distance of items to the cluster center\n",
    "            indices, rtweets, rurls = get_cluster_relevance(texts, vectors, sns, ids)\n",
    "            \n",
    "            # Save or update cluster size\n",
    "            center_size = len(idl)\n",
    "            if new_cluster == True:\n",
    "                center_sizes.append(center_size)\n",
    "            else:\n",
    "                center_sizes[cluster_mod] += center_size\n",
    "\n",
    "            # Get combined sentiment from texts in this cluster\n",
    "            sentiment = get_sentiment(texts)\n",
    "            if new_cluster == True:\n",
    "                center_sentiment.append(sentiment)\n",
    "            else:\n",
    "                center_sentiment[cluster_mod] += sentiment\n",
    "\n",
    "            # Get word frequencies from texts in this cluster\n",
    "            wfreq = get_word_frequencies(texts)\n",
    "            if new_cluster == True:\n",
    "                center_words.append(wfreq)\n",
    "            else:\n",
    "                for x, c in wfreq.most_common():\n",
    "                    center_words[cluster_mod][x] += c\n",
    "            \n",
    "            # Get subject, verb, object triples\n",
    "            svo_triples = get_subject_verb_object_triples(texts)\n",
    "            if new_cluster == True:\n",
    "                center_svo.append(svo_triples)\n",
    "            else:\n",
    "                for x, c in svo_triples.most_common():\n",
    "                    center_svo[cluster_mod][x] += c\n",
    "\n",
    "            # Get common ngrams\n",
    "            ngrams = get_ngram_frequencies(texts, num_grams)\n",
    "            if new_cluster == True:\n",
    "                center_ngrams.append(ngrams)\n",
    "            else:\n",
    "                for x, c in ngrams.most_common():\n",
    "                    center_ngrams[cluster_mod][x] += c\n",
    "\n",
    "            # Get label text (to be shown on graphviz)\n",
    "            if new_cluster == True:\n",
    "                center_label = get_label_text(texts, vectors)\n",
    "                center_labels.append(center_label)\n",
    "\n",
    "            # Add center to the list if it is new (else keep the existing one)\n",
    "            if new_cluster == True:\n",
    "                centers.append(center)\n",
    "\n",
    "            # Add vectors to the list\n",
    "            for v in vectors:\n",
    "                vec_label.append([v, cluster_mod])\n",
    "\n",
    "            # Add or update sn list\n",
    "            snc = Counter(sns)\n",
    "            if new_cluster == True:\n",
    "                center_sns.append(snc)\n",
    "            else:\n",
    "                for x, c in snc.most_common():\n",
    "                    center_sns[cluster_mod][x] += c\n",
    "\n",
    "            # Update or add tweets by relevance\n",
    "            if new_cluster == True:\n",
    "                center_tweets.append(rtweets)\n",
    "            else:\n",
    "                for x, c in rtweets.most_common():\n",
    "                    center_tweets[cluster_mod][x] = c\n",
    "\n",
    "            # Update or add tweet urls by relevance\n",
    "            if new_cluster == True:\n",
    "                center_urls.append(rurls)\n",
    "            else:\n",
    "                for x, c in rurls.most_common():\n",
    "                    center_urls[cluster_mod][x] = c\n",
    "        else:\n",
    "            # All stuff that was thrown away is added to the unclustered list\n",
    "            new_ncindices.extend(idl)\n",
    "\n",
    "    # Print some status output\n",
    "    total_processed += (len(ncindices) - len(new_ncindices))\n",
    "    if len(center_sizes) > 0:\n",
    "        mean_size = np.mean(center_sizes)\n",
    "        smallest = min(center_sizes)\n",
    "        largest = max(center_sizes)\n",
    "        msg = \"Pass: \" + str(passes)\n",
    "        if get_random == False:\n",
    "            msg += \"  I:\" + str(current_index)\n",
    "        msg += \"  N:\" + str(total_processed) \n",
    "        msg += \"  C:\" + str(mod_num)\n",
    "        msg += \" Mean : \" + \"%.2f\"%mean_size\n",
    "        msg += \" Min: \" + str(smallest)\n",
    "        msg += \" Max: \" + str(largest)\n",
    "        msg += \" Merges: \" + str(merges)\n",
    "        print(msg)\n",
    "    ncindices = list(new_ncindices)\n",
    "    passes += 1\n",
    "    if total_processed > total_samples:\n",
    "        finished = True\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell writes a readable text file containing a summary of the clustering process \n",
    "print(\"Writing summary. You can read it by opening tweet_graph_analysis_output.txt\")\n",
    "with open(\"tweet_graph_analysis_output.txt\", \"w\") as f:\n",
    "    for index in range(len(centers)):\n",
    "        ngrams = center_ngrams[index]\n",
    "        wfreq = center_words[index]\n",
    "        svo = center_svo[index]\n",
    "        sentiment = center_sentiment[index]\n",
    "        tweeted = center_sns[index]\n",
    "        size = center_sizes[index]\n",
    "        tweets = center_tweets[index]\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Cluster: \" + str(index) + \" contains: \" + str(size) + \" tweets.\\n\")\n",
    "        f.write(\"Sentiment: \" + \"%.2f\"%sentiment+\"\\n\")\n",
    "        wft = get_wft(wfreq)\n",
    "        f.write(\"Words: \" + wft + \"\\n\")\n",
    "        svoft = get_wft(svo)\n",
    "        f.write(\"svo: \"+svoft+\"\\n\")\n",
    "        nft = get_wft(ngrams)\n",
    "        f.write(\"ngrams: \" + nft+\"\\n\")\n",
    "        snt = print_counter_summary(tweeted)\n",
    "        f.write(\"tweeted: \" + snt +\"\\n\")\n",
    "        f.write(\"==================\\n\")\n",
    "        tt = []\n",
    "        for x, c in tweets.most_common():\n",
    "            tt.append([c, x])\n",
    "        for t in tt[:20]:\n",
    "            f.write(\"%.3f\"%t[0] + \" \" + t[1] +\"\\n\")\n",
    "        if len(tt) > 20:\n",
    "            f.write(\"...\\n\")\n",
    "            for t in tt[-5:]:\n",
    "                f.write(\"%.3f\"%t[0] + \" \" + t[1] +\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This writes out a file that can be read by gephi\n",
    "# WARNING! Consider commenting this out if you're planning on clustering a huge amount of tweets!\n",
    "write_gexf(final_mapping, \"tweet_mapping.gexf\", item_mod, [\"community\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data for subsequent analysis\n",
    "full = {}\n",
    "full[\"ngrams\"] = center_ngrams\n",
    "full[\"words\"] = center_words\n",
    "full[\"svo\"] = center_svo\n",
    "full[\"sentiment\"] = center_sentiment\n",
    "full[\"sns\"] = center_sns\n",
    "full[\"sizes\"] = center_sizes\n",
    "full[\"tweets\"] = center_tweets\n",
    "full[\"urls\"] = center_urls\n",
    "full[\"centers\"] = centers\n",
    "save_bin(full, \"clustering_data.pkl\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function extracts cluster ids containing search terms\n",
    "# and displays details about the top 5 most relevant clusters\n",
    "terms = [\"liar\", \"criminal\", \"idiot\", \"fool\", \"ignorant\", \"delusional\"]\n",
    "found = Counter()\n",
    "for index in range(len(full[\"tweets\"])):\n",
    "    for x, c in full[\"tweets\"][index].most_common():\n",
    "        for term in terms:\n",
    "            if term in x:\n",
    "                found[index] += 1\n",
    "print(\"Found \" + str(len(found)) + \" clusters contained the terms: \\\"\" + \", \".join(terms) + \"\\\".\")\n",
    "\n",
    "cluster_per = Counter()\n",
    "cluster_matches = Counter()\n",
    "for x, c in found.most_common():\n",
    "    size = full[\"sizes\"][x]\n",
    "    matches = c\n",
    "    per = (matches/size) * 100\n",
    "    cluster_per[x] = per\n",
    "    cluster_matches[x] = matches\n",
    "\n",
    "targets = [x for x, c in cluster_per.most_common(5)]\n",
    "print()\n",
    "for t in targets:\n",
    "    msg = \"Cluster \" + str(t) + \" (size \" + str(full[\"sizes\"][t]) + \") contained \" \n",
    "    msg += str(cluster_matches[t]) + \" tweets (\" + \"%.2f\"%cluster_per[t] + \"%) that included the terms: \\\"\" \n",
    "    msg += \", \".join(terms) + \"\\\".\"\n",
    "    print(msg)\n",
    "    tm = \"\"\n",
    "    tc = 0\n",
    "    for x, c in full[\"words\"][t].most_common():\n",
    "        if x not in stopwords:\n",
    "            tm += x + \"(\" + str(c) + \") \"\n",
    "            tc += 1\n",
    "        if tc >= 10:\n",
    "            break\n",
    "    print(tm)\n",
    "    print()\n",
    "    for x, c in full[\"tweets\"][t].most_common(10):\n",
    "        print(\"%.3f\"%c + \": \" + x)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
