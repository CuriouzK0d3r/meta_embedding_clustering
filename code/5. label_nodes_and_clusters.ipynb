{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data = load_bin(\"clustering_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent(text):\n",
    "    sents = []\n",
    "    blob = TextBlob(text)\n",
    "    for sentence in blob.sentences:\n",
    "        sents.append(sentence.sentiment.polarity)\n",
    "    return np.sum(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Attempt to label each center\n",
    "# Also, calculate sentiment of the derived label\n",
    "# and compare it to sentiment derived from all tweets in the cluster\n",
    "# This is then used to calculate positive, negative, and toxic categories\n",
    "num_grams = 20\n",
    "num_words = 10\n",
    "\n",
    "node_label = {}\n",
    "node_sent = {}\n",
    "toxic = 0\n",
    "negative = 0\n",
    "positive = 0\n",
    "for index in range(len(cluster_data[\"centers\"])):\n",
    "    words = Counter()\n",
    "    # Get a list of important non-stop words from tweets\n",
    "    while len(words) < num_words:\n",
    "        for x, c in cluster_data[\"words\"][index].most_common():\n",
    "            if x not in stopwords:\n",
    "                words[x] = c\n",
    "    # Get a list of the most common ngrams and svo triplets\n",
    "    # reformat svo triplets to be same as ngrams\n",
    "    summaries = Counter()\n",
    "    for x, c in cluster_data[\"svo\"][index].most_common(num_grams):\n",
    "        x = \" \".join(x[1:-1].split(\", \"))\n",
    "        summaries[x] += c\n",
    "    for x, c in cluster_data[\"ngrams\"][index].most_common(num_grams):\n",
    "        summaries[x] += c\n",
    "    # If a word is found in a summary, assign it to potential labels counter\n",
    "    # with count equal to the frequency of the matched word\n",
    "    labels = Counter()\n",
    "    for word, count in words.most_common(num_words):\n",
    "        for x, c in summaries.most_common(num_grams):\n",
    "            if word in x:\n",
    "                labels[x] += count\n",
    "    # Create bigrams of commonly seen words\n",
    "    # If a bigram is seen in a summary, assign it to potential labels counter\n",
    "    # with count equal to the value of the word in the bigram with the highest frequenct\n",
    "    word_combs = combinations([x for x, c in words.most_common(num_words)], 2)\n",
    "    for comb in word_combs:\n",
    "        fc = max([words[x] for x in comb])\n",
    "        ws = \" \".join(comb)\n",
    "        for x, c in summaries.most_common(num_grams):\n",
    "            if ws in x:\n",
    "                labels[x] += fc\n",
    "    # The top item found in labels is the summary\n",
    "    # Note how we add the node index to the label\n",
    "    # This is because some labels are identical\n",
    "    # and thus cause the gephi creation step in a future cell\n",
    "    # to miss nodes\n",
    "    top_label = \"\"\n",
    "    for x, c in labels.most_common(1):\n",
    "        top_label = \"[\" + str(index) + \"] \" + x\n",
    "    # This is used to label the item in gephi or other visualizations\n",
    "    node_label[index] = top_label\n",
    "    # Get the sentiment score of the label\n",
    "    sent = get_sent(top_label)\n",
    "    # Get the average sentiment score of the tweets in this cluster\n",
    "    size = len(cluster_data[\"tweets\"][index])\n",
    "    tsent = cluster_data[\"sentiment\"][index]/size\n",
    "    # Assign verdicts based on sentiment analysis\n",
    "    # the values were hand-adjusted based on manual inspection\n",
    "    # of tweets in each cluster\n",
    "    verd = \"\"\n",
    "    if tsent < -0.1:\n",
    "        node_sent[index] = 0\n",
    "        verd = \"TOXIC\\t\"\n",
    "        toxic += size\n",
    "    elif tsent < 0.1:\n",
    "        node_sent[index] = 1\n",
    "        verd = \"NEGATIVE\"\n",
    "        negative += size\n",
    "    elif tsent > 0.1:\n",
    "        node_sent[index] = 2\n",
    "        verd = \"POSITIVE\"\n",
    "        positive += size\n",
    "\n",
    "    # Print the results\n",
    "    print(\"(\" + str(size) + \")\\t[\" + \"%.2f\"%sent + \"]\\t\" + \"%.2f\"%tsent + \"\\t\" + verd + \"\\t\" + top_label )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a breakdown of categories\n",
    "tot = positive+negative+toxic\n",
    "posper = positive/tot*100\n",
    "negper = negative/tot*100\n",
    "toxper = toxic/tot*100\n",
    "msg = \"Positive: \" + str(positive) + \" (\" + \"%.2f\"%posper + \"%)\"\n",
    "msg += \" Negative: \" + str(negative) + \" (\" + \"%.2f\"%negper + \"%)\"\n",
    "msg += \" Toxic: \" + str(toxic) + \" (\" + \"%.2f\"%toxper + \"%)\"\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a cluster defined by target variable\n",
    "# This cell prints tweets that don't contain terms identified\n",
    "# during the previous labeling operation\n",
    "# This is useful for manually inspecting the cluster to determine\n",
    "# whether the rest of the tweets are similar in topic or context\n",
    "target = 7\n",
    "print(\"Cluster: \" + node_label[target] + \" contains \" + str(len(cluster_data[\"tweets\"][target])) + \" tweets.\")\n",
    "terms = []\n",
    "for word in node_label[target][4:].split():\n",
    "    if word not in stopwords:\n",
    "        terms.append(word)\n",
    "print(terms)\n",
    "print()\n",
    "found = 0\n",
    "for x, c in cluster_data[\"tweets\"][target].most_common():\n",
    "    matches = 0\n",
    "    for t in terms:\n",
    "        if t in x:\n",
    "            matches += 1\n",
    "    if matches == 0 :\n",
    "        print(\"%.2f\"%c + \"\\t\" + x)\n",
    "        found += 1\n",
    "print(\"Found: \" + str(found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the clusters!\n",
    "# This allows us to visualize the resulting data in gephi\n",
    "# This is also what was used to create the interactive demo\n",
    "# https://twitter-clustering.web.app/\n",
    "centers = cluster_data[\"centers\"]\n",
    "center_clusters, center_mapping = make_text_clusters(centers, edge_ratio=20)\n",
    "com_counts = [len(c) for x, c in center_clusters.items()]\n",
    "print(\"Communities: \" + str(len(center_clusters)) + \": \" + str(com_counts))\n",
    "nodes = set()\n",
    "for m, x in center_clusters.items():\n",
    "    nodes.update(x)\n",
    "print(\"Nodes: \" + str(len(nodes)))\n",
    "print(\"Edges: \" + str(len(center_mapping)))\n",
    "\n",
    "nodes_json = {}\n",
    "center_node_attr = {}\n",
    "for mod, nodes in center_clusters.items():\n",
    "    for n in nodes:\n",
    "        label = \"n\" + str(n)\n",
    "        summary = node_label[n]\n",
    "        sent = node_sent[n]\n",
    "        size = cluster_data[\"sizes\"][n]\n",
    "        center_node_attr[summary] = [mod, size, sent]\n",
    "        nodes_json[label] = {}\n",
    "        nodes_json[label][\"label\"] = node_label[n]\n",
    "        nodes_json[label][\"community\"] = mod\n",
    "        nodes_json[label][\"sentiment\"] = cluster_data[\"sentiment\"][n]\n",
    "        nodes_json[label][\"wfreq\"] = cluster_data[\"words\"][n]\n",
    "        nodes_json[label][\"words\"] = get_wft(cluster_data[\"words\"][n])\n",
    "        nodes_json[label][\"ngrams\"] = get_wft(cluster_data[\"ngrams\"][n])\n",
    "        nodes_json[label][\"svo\"] = get_wft(cluster_data[\"svo\"][n])\n",
    "        nodes_json[label][\"tweeted\"] = print_counter_summary(cluster_data[\"sns\"][n])\n",
    "        nodes_json[label][\"size\"] = cluster_data[\"sizes\"][n]\n",
    "        nodes_json[label][\"tweets\"] = [x for x, c in cluster_data[\"tweets\"][n].most_common(10)]\n",
    "        nodes_json[label][\"urls\"] = [x for x, c in cluster_data[\"urls\"][n].most_common(10)]\n",
    "\n",
    "with open(\"edges.csv\", \"w\") as f:\n",
    "    f.write(\"Sourceid,Targetid,Weight\\n\")\n",
    "    for m in center_mapping:\n",
    "        s, t, w = m\n",
    "        f.write(\"n\"+str(s)+\",n\"+str(t)+\",\"+str(w)+\"\\n\")\n",
    "\n",
    "geph_mapping = []\n",
    "for item in center_mapping:\n",
    "    s, t, w = item\n",
    "    s1 = node_label[s]\n",
    "    t1 = node_label[t]\n",
    "    geph_mapping.append([s1, t1, w])\n",
    "\n",
    "save_json(nodes_json, \"nodes.json\")\n",
    "write_gexf(geph_mapping, \"center_mapping.gexf\", center_node_attr, [\"community\", \"size\", \"sentiment\"])\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print out some information about each cluster\n",
    "# Most common words, ngrams, svo triplets\n",
    "cluster_words = {}\n",
    "cluster_ngrams = {}\n",
    "cluster_svo = {}\n",
    "for mod, nodes in center_clusters.items():\n",
    "    cluster_words[mod] = Counter()\n",
    "    cluster_ngrams[mod] = Counter()\n",
    "    cluster_svo[mod] = Counter()\n",
    "    for n in nodes:\n",
    "        words = cluster_data[\"words\"][n]\n",
    "        ngrams = cluster_data[\"ngrams\"][n]\n",
    "        svo = cluster_data[\"svo\"][n]\n",
    "        for x, c in words.items():\n",
    "            if x not in stopwords:\n",
    "                cluster_words[mod][x] += c\n",
    "        for x, c in ngrams.items():\n",
    "            cluster_ngrams[mod][x] += c\n",
    "        for x, c in svo.items():\n",
    "            cluster_svo[mod][x] += c\n",
    "# Print top words\n",
    "print(\"Words\")\n",
    "for mod, words in cluster_words.items():\n",
    "    top_words = \" \"\n",
    "    for x, c in words.most_common(10):\n",
    "        top_words += x + \"(\" + str(c) + \") \"\n",
    "    \n",
    "    print(\"Community \" + str(mod) + top_words)\n",
    "print()\n",
    "# Print top ngrams\n",
    "print(\"ngrams\")\n",
    "for mod, ngrams in cluster_ngrams.items():\n",
    "    top_ngrams = \" \"\n",
    "    for x, c in ngrams.most_common(5):\n",
    "        top_ngrams += x + \"(\" + str(c) + \") \"\n",
    "    \n",
    "    print(\"Community \" + str(mod) + top_ngrams)\n",
    "print()\n",
    "# Print top svo\n",
    "print(\"svo\")\n",
    "for mod, svo in cluster_svo.items():\n",
    "    top_svo = \" \"\n",
    "    for x, c in svo.most_common(5):\n",
    "        top_svo += x + \"(\" + str(c) + \") \"\n",
    "    \n",
    "    print(\"Community \" + str(mod) + top_svo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to find best label for each community\n",
    "# This uses a similar method to one above, but with different parameters\n",
    "num_grams = 10\n",
    "num_words = 10\n",
    "for mod, svo in cluster_svo.items():\n",
    "    words = cluster_words[mod]\n",
    "    ngrams = cluster_ngrams[mod]\n",
    "    relevant_svo = Counter()\n",
    "    for word, count in words.most_common(num_words):\n",
    "        for x, c in svo.most_common(num_grams):\n",
    "            x = \" \".join(x[1:-1].split(\", \"))\n",
    "            if word in x:\n",
    "                relevant_svo[x] += count\n",
    "        for x, c in ngrams.most_common(num_grams):\n",
    "            if word in x:\n",
    "                relevant_svo[x] += count\n",
    "    word_combs = combinations([x for x, c in words.most_common(num_words)], 2)\n",
    "    for comb in word_combs:\n",
    "        fc = max([words[x] for x in comb])\n",
    "        ws = \" \".join(comb)\n",
    "        for x, c in svo.most_common(num_grams):\n",
    "            x = \" \".join(x[1:-1].split(\", \"))\n",
    "            if ws in x:\n",
    "                relevant_svo[x] += fc\n",
    "        for x, c in ngrams.most_common(num_grams):\n",
    "            if ws in x:\n",
    "                relevant_svo[x] += fc\n",
    "    \n",
    "    top_svo = \"\"\n",
    "    for x, c in relevant_svo.most_common(3):\n",
    "        top_svo += x + \" (\" + str(c) + \") \"\n",
    "    top_svo = [x for x, c in relevant_svo.most_common(1)][0]\n",
    "    \n",
    "    print(\"Community \" + str(mod) + \": \" + top_svo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
