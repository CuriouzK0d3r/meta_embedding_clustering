{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "def make_gram_vec(vec_slice):\n",
    "    gv = []\n",
    "    divisor = 0\n",
    "    for i, v in enumerate(vec_slice):\n",
    "        gv.append(v * (i+1))\n",
    "        divisor += (i+1)\n",
    "    gram_vec = np.sum(np.array(gv), axis=0)\n",
    "    gram_vec = gram_vec/divisor\n",
    "    return gram_vec\n",
    "\n",
    "def sentence_vector_ngram(vecs, num_grams):\n",
    "    if num_grams == 1:\n",
    "        return np.sum(np.array(vecs), axis=0)\n",
    "    gram_vecs = []\n",
    "    for i in range(0, len(vecs) - (num_grams-1)):\n",
    "        gram_vec = make_gram_vec(vecs[i:i+num_grams])\n",
    "        gram_vecs.append(gram_vec)   \n",
    "    final = np.sum(np.array(gram_vecs), axis=0)\n",
    "    return final\n",
    "\n",
    "def calculate_sentence_vector(vecs):\n",
    "    num_grams = 4\n",
    "    return sentence_vector_ngram(vecs, num_grams)\n",
    "\n",
    "def get_sentence_vectors(tokens, model):\n",
    "    vocab_set = set(list(model.wv.vocab.keys()))\n",
    "    vecsindexed = []\n",
    "    for clean_tokens in tokens:\n",
    "        vecs = []\n",
    "        for token in clean_tokens:\n",
    "            if token in vocab_set:\n",
    "                vecs.append(model.wv[token])\n",
    "        final = calculate_sentence_vector(vecs)\n",
    "        vecsindexed.append(final)\n",
    "    return vecsindexed\n",
    "\n",
    "def make_word2vec_model(sentences):\n",
    "    params = {'min_count': 3, 'window': 5, 'sample': 0.001, 'sg': 0, 'negative': 5, 'num_features': 768}\n",
    "    print(\"w2v training data contained: \" + str(len(sentences)) + \" sentences.\")\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "    sentence_count = len(sentences)\n",
    "\n",
    "    word2vec = w2v.Word2Vec(sg=params[\"sg\"],\n",
    "                            seed=1,\n",
    "                            workers=num_workers,\n",
    "                            size=params[\"num_features\"],\n",
    "                            min_count=params[\"min_count\"],\n",
    "                            window=params[\"window\"],\n",
    "                            sample=params[\"sample\"])\n",
    "\n",
    "\n",
    "    word2vec.build_vocab(sentences)\n",
    "    print(\"Training model with vocabulary length:\", len(word2vec.wv.vocab))\n",
    "    epoch_count = 10\n",
    "    word2vec.train(sentences, total_examples=sentence_count, epochs=epoch_count)\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "\n",
    "def make_doc2vec_model(sentences):\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "    sentence_count = len(sentences)\n",
    "\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(sentences)]\n",
    "    alpha = 0.025\n",
    "    doc2vec = Doc2Vec(documents,\n",
    "                      dm=1,\n",
    "                      alpha=alpha,\n",
    "                      min_alpha=0.00025, \n",
    "                      window=2, \n",
    "                      min_count=1,\n",
    "                      vector_size=768,\n",
    "                      workers=num_workers)\n",
    "    print(\"Training model with vocabulary length:\", len(doc2vec.wv.vocab))\n",
    "    \n",
    "    epoch_count = 10\n",
    "    print(\"Training doc2vec\")\n",
    "    doc2vec.train(documents, total_examples=sentence_count, epochs=epoch_count)\n",
    "    return doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load texts and tokens created by preprocess_and_tokenize_tweets.ipynb\n",
    "prefix = \"\"\n",
    "print(\"Loading tokens\")\n",
    "tokens = load_json(\"preprocessed/\" + prefix + \"tokens.json\")\n",
    "print(len(tokens))\n",
    "print(\"Loading texts\")\n",
    "texts = load_json(\"preprocessed/\" + prefix + \"texts.json\")\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make w2v vectors\n",
    "print(\"Building w2v model\")\n",
    "w2v_model = make_word2vec_model(tokens)\n",
    "print(\"Built\")\n",
    "print(\"Converting texts to vectors\")\n",
    "w2v_vectors = get_sentence_vectors(tokens, w2v_model)\n",
    "print(len(w2v_vectors))\n",
    "w2v_text_vec = {}\n",
    "for i, t in enumerate(texts):\n",
    "    w2v_text_vec[t] = w2v_vectors[i]\n",
    "print(\"Saving\")\n",
    "save_bin(w2v_text_vec, \"preprocessed/\" + prefix + \"word2vec_text_vec.pkl\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make d2v vectors\n",
    "print(\"Building d2v model\")\n",
    "d2v_model = make_doc2vec_model(tokens)\n",
    "print(\"Model built\")\n",
    "d2v_text_vec = {}\n",
    "for i, text in enumerate(texts):\n",
    "    vec = d2v_model.docvecs[i]\n",
    "    d2v_text_vec[text] = vec\n",
    "print(\"Saving\")\n",
    "save_bin(d2v_text_vec, \"preprocessed/\" + prefix + \"doc2vec_text_vec.pkl\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "print(\"Instantiating BERT model\")\n",
    "bert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# Since converting sentences to bert vectors is time-consuming\n",
    "# we load the existing vectors and obtain a list of those sentences\n",
    "# that haven't yet been converted, so that we don't have to convert\n",
    "# all every time we run this.\n",
    "# Converting 100,000 sentences takes about an hour\n",
    "old_bert_text_vec = {}\n",
    "print(\"Loading existing vectors\")\n",
    "if os.path.exists(\"preprocessed/bert_text_vec.pkl\"):\n",
    "    old_bert_text_vec = load_bin(\"preprocessed/\" + prefix + \"bert_text_vec.pkl\")\n",
    "    print(len(old_bert_text_vec))\n",
    "already_processed = set([x for x, v in old_bert_text_vec.items()])\n",
    "not_processed = list(set(texts).difference(already_processed))\n",
    "print(\"Not processed: \" + str(len(not_processed)))\n",
    "\n",
    "# This encodes the vectors. There is no output during the process\n",
    "# so just be patient\n",
    "bert_vectors = bert_model.encode(not_processed)\n",
    "print(\"Vectors encoded\")\n",
    "\n",
    "# Combine newly created bert vectors with those that were already saved\n",
    "for i, t in enumerate(not_processed):\n",
    "    old_bert_text_vec[t] = bert_vectors[i]\n",
    "bert_text_vec = {}\n",
    "for t in texts:\n",
    "    bert_text_vec[t] = old_bert_text_vec[t]\n",
    "\n",
    "# Save the new set\n",
    "print(\"Saving vectors\")\n",
    "save_bin(bert_text_vec, \"preprocessed/\" + prefix + \"bert_text_vec.pkl\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the three sentence vectors into meta embeddings and save for next step\n",
    "combined_vecs = []\n",
    "for i, text in enumerate(texts):\n",
    "    if i % 100000 == 0:\n",
    "        print(i)\n",
    "    bert_vec = bert_text_vec[text]\n",
    "    d2v_vec = d2v_text_vec[text]\n",
    "    w2v_vec = w2v_text_vec[text]\n",
    "    combined = np.sum([bert_vec, d2v_vec, w2v_vec], axis=0)\n",
    "    combined_vecs.append(combined)\n",
    "print(\"Saving\")\n",
    "save_bin(combined_vecs, \"preprocessed/\" + prefix + \"combined_vecs.pkl\")\n",
    "print(\"Done. Now execute tweet_graph_analysis.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
